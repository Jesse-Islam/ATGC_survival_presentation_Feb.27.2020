---
title: "Absolute Risk integration using penalized logistic regression"
author: "Jesse Islam"
date: "2/16/2020"
mathspec: true
theme: "metropolis"
output:
  beamer_presentation:
    incremental: yes
    fig_caption: false
---
```{r packages,include=FALSE,echo=FALSE}

source("packages.R")
set.seed(1)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(include = FALSE, warning=FALSE,message=FALSE)
knitr::opts_chunk$set(dev = 'pdf')
```

## Popular methods in time-to-event analysis 

* In disease etiology, we tend to make use of the proportional hazards hypothesis.
    * Cox Regression

* When we want the absolute risk:
    * Breslow estimator
    * Parametric models
    
## Motivations for a new method
* Julien and Hanley found that survival analysis rarely produces prognostic functions, even though the software is widely available in cox regression packages. [1]
* They believe the stepwise nature is the reason, as it reduces interpretability. [1]
* Want to easily model non-proportional hazards. [1]
* A streamlined approach for reaching a **smooth absolute risk** curve. [1]


## Dr. Cox's perspective
![](coxquote.png)

## Index
* SUPPORT study
* Casebase sampling
* Logistic regression on survival data
* Maximum likelihood with regularization
* Comparing hazard models in SUPPORT study
* Absolute risk comparison
* Future work
* References

## SUPPORT dataset
* **Study to Understand Prognoses and Preferences for Outcomes and Risks Treatments**
* Design: Prospective cohort study.
* Setting: 5 academic care centers in the United States.
* Participants: 9105 hospitalized.
* Follow-up-time: 5.56 years.
* 68% incidence rate.

## SUPPORT manual imputation
* Notorious for missing data
\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ll}
\hline
Baseline Variable      & Normal Fill-in Value \\ \hline
Bilirubin              & 1.01                 \\
BUN                    & 6.51                 \\
Creatinine             & 1.01                 \\
PaO2/FiO2 ratio (pafi) & 333.3                \\
Serum albumin          & 3.5                  \\
Urine output           & 2502                 \\
White blood count      & 9 (thousands)        \\ \hline
\end{tabular}%
}
\caption{Suggested imputation values. {[}Support site reference{]}}
\label{tab:my-table}
\end{table}

## SUPPORT automated imputation

* mice imputation package (R)

1. PMM (Predictive Mean Matching)  – For numeric variables
2. logreg(Logistic Regression) – For Binary Variables( with 2 levels)
3. polyreg(Bayesian polytomous regression) – For Factor Variables (>= 2 levels)
4. Proportional odds model (ordered, >= 2 levels)

## Removed variables

* Hospital Charges.
* Patient ratio of costs to charges.
* Patient Micro-costs.
* Ordinal functional disability.
* Income (ordinal).

## Variable overview

* Age, sex, race, education, follow-up time, death. (6)
* Disease group/class, Number of comorbidities. (3)
* Income, costs. (4)
* Coma score, average Therapeutic Intervention Scoring System (2)
* Physiological variables. (11)
* Activities of daily living. (3)
* Previous model findings. (8)

## Original SUPPORT analysis

* Determined SUPPORT prognostic model on phase I (4301 individuals).
* Tested on Phase II (4028 individuals).
* Both on the scale of 180 days.
* Write out complicated model?????
* image of SPS vs APS ???????

## SUPPORT question

* How does their model perform over 5.56 years?
* Absolute Risk comparison.

## Analysis Process

1. Impute
2. Compare SPS and APS over ~5.56 years using absolute risk curves.
3. Compare to Kaplan-Meier curve
4. Compare to full model (excluding SPS and APS)

* All models is trained on 80% of the observations.
* Remaining observations are used to generate comparative absolute risk curves.

```{r data processing}
ew <- read_csv("support2.csv")
data<-read.csv('support2.csv', col.names=colnames(ew), fill=TRUE, header=TRUE)
rm(ew)
```


```{r cleaningImputation}
#manual imputation
data$alb[is.na(data$alb)]=3.5
data$pafi[is.na(data$pafi)]=333.3
data$bili[is.na(data$bili)]=1.01
data$crea[is.na(data$crea)]=1.01
data$bun[is.na(data$bun)]=6.51
data$wblc[is.na(data$wblc)]=9
data$urine[is.na(data$urine)]=2502

# automated imputation
previousModelPositions<-c(18,19,20,21,22,23,24,25,26,27,28,29)

supportMain<-data[,-previousModelPositions]
supportPrevious<-data[,previousModelPositions]
#keep 1,2 after imputation from previous usable models.

#missingPattern<-md.pattern(supportMain)
supportMainImpute <-mice(supportMain[,-c(11,34,13,14,15)], m = 1) #impute while removing ordinal variables and other response variables.
imputedData<-cbind(complete(supportMainImpute,1),supportPrevious[,c(1,2)],printFlag=FALSE)
#adls is missing 33%, and is removed accordingly and hospitaldeath
#md.pattern(completeData)
completeData<-na.omit(imputedData[,-c(4,29)])
```

```{r supportSetUp, echo = FALSE, eval = eval_cs3}

ratio=10  #universal ratio for eval_cs3
#keep one individual out, at random, for which we will develop an 
#absolute risk curve.
#sam=sample(1:nrow(completeData), )
#Set workingData to the remaining individuals in the data
#workingData=completeData[-c(sam),]

workingCompleteData=model.matrix(death~ .-d.time,data=completeData)[,-c(1)]#remove intercept


#Create u and xCox, which will be used when fitting Cox with glmnet.
newData=workingCompleteData
x=workingCompleteData
#x and y will be used to fit the casebase model under glmnet.
#x=as.matrix(sparse.model.matrix(death~ .-d.time+0,data=workingData))
y=data.matrix(completeData[,c(2,5)])
```


```{r CoxExplorationModels, echo = TRUE, eval = eval_cs3}

fullDataCox<-as.data.frame(cbind(as.numeric(y[,2]),as.numeric(y[,1]),x))
train_index <- sample(1:nrow(fullDataCox), 0.8 * nrow(fullDataCox))
test_index <- setdiff(1:nrow(fullDataCox), train_index)

train<-fullDataCox[train_index,]
test<-fullDataCox[test_index,]
coxSPS<- survival::coxph(Surv(time=V1,event=V2) ~ sps, data = train)
abcoxSPS<-survival::survfit(coxSPS,newdata = test)

coxAPS<- survival::coxph(Surv(time=V1,event=V2) ~ aps, data = train)
abcoxAPS<-survival::survfit(coxAPS,newdata = test)

coxFull<- survival::coxph(Surv(time=V1,event=V2) ~ .-sps -aps, data = train)
abcoxFull<-survival::survfit(coxAPS,newdata = test)

KM<- survival::coxph(Surv(time=V1,event=V2) ~ 1, data = test)
abKM<-survival::survfit(KM)
abKMcompare<-cbind(abKM$time[abKM$time<=1631],1-abKM$surv[abKM$time<=1631])
colnames(abKMcompare)<-c("Time","unadjusted")

baseCoxComparisonData<-cbind(c(1:1634),1-rowMeans(abcoxSPS$surv),1-rowMeans(abcoxAPS$surv),1-abKM$surv,1-rowMeans(abcoxFull$surv))
colnames(baseCoxComparisonData)<-c("Time","SPS","APS","unadjusted","Full")
library(RColorBrewer)
myColors <- brewer.pal(5,"Dark2")
```


## SPS vs APS
```{r SPSvsAPS}
#Full
ggplot(as.data.frame(baseCoxComparisonData), aes(Time)) + 
  geom_line(aes(y = APS, colour = "APS"),lwd=2) +
  geom_line(aes(y = SPS, colour = "SPS"),lwd=2) +
  scale_colour_manual(name = "Models",values = myColors[c(3,4)])
```

## SPS vs APS
```{r SPSvsAPSZoom}
#ZOOM
ggplot(as.data.frame(baseCoxComparisonData), aes(Time)) + 
  geom_line(aes(y = APS, colour = "APS"),lwd=2) +
  geom_line(aes(y = SPS, colour = "SPS"),lwd=2) + 
 scale_x_continuous(limits = c(1000, 1250))+
  scale_colour_manual(name = "Models",values = myColors[c(3,4)])
```


## SPS vs. Kaplan-Meier
```{r SPSvsKM}
ggplot(as.data.frame(baseCoxComparisonData), aes(Time)) + 
  geom_line(aes(y = APS, colour = "APS"),lwd=2) +
  geom_line(aes(y = SPS, colour = "SPS"),lwd=2) +
  geom_step(data=as.data.frame(abKMcompare),mapping=aes(x=Time,y=KM,color="unadjusted"),lwd=2)+
  scale_colour_manual(name = "Models",values = myColors[c(3,2,4)])
```

## SPS vs. Kaplan-Meier
```{r SPSvsKMZoom}
ggplot(as.data.frame(baseCoxComparisonData), aes(Time)) + 
  geom_line(aes(y = APS, colour = "APS"),lwd=2) +
  geom_line(aes(y = SPS, colour = "SPS"),lwd=2) +
  geom_step(data=as.data.frame(abKMcompare),mapping=aes(x=Time,y=KM,color="unadjusted"),lwd=2)+
  scale_x_continuous(limits = c(1000, 1250))+
  scale_colour_manual(name = "Models",values = myColors[c(3,2,4)])
```


## All covariates vs. physiology scores vs unadjusted
```{r FullvsKM}
ggplot(as.data.frame(baseCoxComparisonData), aes(Time)) + 
  geom_line(aes(y = APS, colour = "APS"),lwd=2) +
  geom_line(aes(y = SPS, colour = "SPS"),lwd=2) + 
  geom_step(data=as.data.frame(abKMcompare),mapping=aes(x=Time,y=KM,color="unadjusted"),lwd=2)+
  geom_line(aes(y = Full, col = "All Covariates without SPS APS"),lwd=2)+
  scale_colour_manual(name = "Models",values = myColors[c(1,3,2,4)])
```

* Conclusion: linear associations without physiology scores performs similarly to physiology scores. (SPS and APS) (USE IPA)
* 


```{r}
ggplot(as.data.frame(baseCoxComparisonData), aes(Time)) + 
  geom_step(data=as.data.frame(abKMcompare),mapping=aes(x=Time,y=KM,color="unadjusted"),lwd=2)+
  geom_line(aes(y = Full, colour = "Full"),lwd=2)+
  scale_colour_manual(name = "Models",values = myColors[c(1,2)])

```

```{r cbHazardInteractions}, echo = TRUE, eval = eval_cs3}

cbFull=casebase::fitSmoothHazard.fit(x[train_index,],y[train_index,],family="glmnet",time="d.time",event="death",formula_time = ~log(d.time),alpha=1,ratio=ratio)
```
Then, we take our model and use the absoluteRisk function to integrate and retrieve our absolute risk function.


```{r CBabsoluteInteracts, echo = TRUE, eval = eval_cs3}
#Estimating the absolute risk curve using the newData parameter.
abCbFull=casebase::absoluteRisk(cbFull,time = seq(1,max(completeData$d.time), 400),newdata = x[test_index,] , s="lambda.1se",method=c("numerical"))

cbAbs=data.frame(casebaseAbsoluteInter[,1],rowMeans(casebaseAbsoluteInter[,-c(1)]))
```



## Casebase Overview

1. Clever sampling.
2. Implicitly deals with censoring.
3. Allows a parametric fit using *logistic regression*.

* Casebase is parametric, and allows different parametric fits by incorporation of the time component.
* Package contains an implementation for generating *population-time* plots.

## Casebase: Sampling

```{r , echo=FALSE, warning=FALSE,message=FALSE}
ERSPC=casebase::ERSPC

ERSPC$ScrArm <- factor(ERSPC$ScrArm, 
                       levels = c(0,1), 
                       labels = c("Control group", "Screening group"))
nobs <- nrow(ERSPC)
ftime <- ERSPC$Follow.Up.Time
ord <- order(ftime, decreasing = FALSE)
yCoords <- cbind(cumsum(ERSPC[ord, "DeadOfPrCa"] == 1),
cumsum(ERSPC[ord, "DeadOfPrCa"] == 0))
yCoords <- cbind(yCoords, nobs - rowSums(yCoords))
aspectRatio <- 0.75
height <- 8.5 * aspectRatio; width <- 11 * aspectRatio
cases <- ERSPC[, "DeadOfPrCa"] == 1
comps <- ERSPC[, "DeadOfPrCa"] == 2

# randomly move the cases vertically
moved_cases <- yCoords[cases[ord], 3] * runif(sum(cases))
moved_comps <- yCoords[comps[ord], 3] * runif(sum(comps))
plot(0, type = 'n', xlim = c(0, max(ftime)), ylim = c(0, nobs),
xlab = 'Follow-up time (years)', ylab = 'Population',cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)

```

## Casebase: Sampling
```{r , echo=FALSE, warning=FALSE,message=FALSE}
plot(0, type = 'n', xlim = c(0, max(ftime)), ylim = c(0, nobs),
xlab = 'Follow-up time (years)', ylab = 'Population',cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
polygon(c(0, 0, ftime[ord], max(ftime), 0),
c(0, nobs, yCoords[,3], 0, 0),
col = "grey80")
```

## Casebase: Sampling
```{r , echo=FALSE, warning=FALSE,message=FALSE}
plot(0, type = 'n', xlim = c(0, max(ftime)), ylim = c(0, nobs), legend=TRUE,
xlab = 'Follow-up time (years)', ylab = 'Population',cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
polygon(c(0, 0, ftime[ord], max(ftime), 0),
c(0, nobs, yCoords[,3], 0, 0),
col = "grey80")
points((ftime[ord])[cases[ord]], yCoords[cases[ord],3], pch = 19,
col = "firebrick3", cex = 0.5)
legend("right", legend = c("Event"),
col = c("firebrick3", "dodgerblue2", "black"),
pch = 19,cex=1.5)

```

## Casebase: Sampling
```{r,echo=TRUE,eval=FALSE}
casebase::popTime(Data,Event,Time)
```
```{r , echo=FALSE, warning=FALSE,message=FALSE}

plot(0, type = 'n', xlim = c(0, max(ftime)), ylim = c(0, nobs),
xlab = 'Follow-up time (years)', ylab = 'Population',cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
polygon(c(0, 0, ftime[ord], max(ftime), 0),
c(0, nobs, yCoords[,3], 0, 0),
col = "grey80")
points((ftime[ord])[cases[ord]], moved_cases, pch = 19,
col = "firebrick3", cex = 0.5)
legend("topright", legend = c("Relapse"),
col = c("firebrick3", "dodgerblue2", "black"),
pch = 19,cex=1.5)
death="DeadOfPrCa"
ftime="Follow.Up.Time"
```


## Casebase: Sampling
```{r,echo=FALSE}
ftime <- ERSPC$Follow.Up.Time
plot(0, type = 'n', xlim = c(0, max(ftime)), ylim = c(0, nobs),
xlab = 'Follow-up time (years)', ylab = 'Population',cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
points((ftime[ord])[cases[ord]], yCoords[cases[ord],3], pch = 19,
col = "white", cex = 0.5)
lines(rep((ftime[ord])[cases[ord]], each=3), t(matrix(c(yCoords[cases[ord],3], rep(c(0,NA),each=length(yCoords[cases[ord],3]))), ncol=3)),col="grey80") 
points((ftime[ord])[cases[ord]], moved_cases, pch = 19,
col = "firebrick3", cex = 0.5)
legend("topright", legend = c("Relapse"),
col = c("firebrick3", "dodgerblue2", "black"),
pch = 19,cex=1.5)
ERSPC$ScrArm<-factor(ERSPC$ScrArm)
ERSPC$DeadOfPrCa<-as.numeric(ERSPC$DeadOfPrCa)
```

## Casebase: Parametric families
* We can now fit models of the form:
$$\ log(h(t;\alpha,\beta))=g(t;\alpha)+\beta X$$
* By changing the function $g(t;\alpha)$, we can model different parametric families easily:


## Casebase: Parametric models
*Exponential*: $g(t;\alpha)$ is equal to a constant
```{r,echo=TRUE,eval=FALSE}
casebase::fitSmoothHazard(status ~ X1 + X2)
```
*Gompertz*: $g(t;\alpha)=\alpha t$
```{r,echo=TRUE,eval=FALSE}
casebase::fitSmoothHazard(status ~ time + X1 + X2)
```
*Weibull*: $g(t;\alpha) = \alpha log(t)$
```{r,echo=TRUE,eval=FALSE}
casebase::fitSmoothHazard(status ~ log(time) + X1 + X2)
```


## Death by prostate cancer: hazard ratios
```{r,echo=FALSE, warning=FALSE,message=FALSE}
wModel<-casebase::fitSmoothHazard(DeadOfPrCa ~
log(Follow.Up.Time) + 
ScrArm, data=casebase::ERSPC, ratio = 100)
```
```{r,echo=TRUE, warning=FALSE,message=FALSE,eval=FALSE}
casebase::fitSmoothHazard(DeadOfPrCa~ log(Follow.Up.Time)+ 
                          ScrArm, data=ERSPC, ratio = 100)
```

```{r echo=FALSE, warning=FALSE,message=FALSE, fig.height = 1, fig.width = 1, fig.align = "center",size='tiny',eval=FALSE}
#summary(wModel)
```



## ERSPC Hazard comparison
```{r echo=FALSE, warning=FALSE,message=FALSE}
round_df <- function(x, digits) {
    # round all numeric variables
    # x: data frame 
    # digits: number of digits to round
    numeric_columns <- sapply(x, mode) == 'numeric'
    x[numeric_columns] <-  round(x[numeric_columns], digits)
    x
}
ERSPC$ScrArm<-factor(ERSPC$ScrArm)
gModel <- casebase::fitSmoothHazard(DeadOfPrCa ~ Follow.Up.Time + ScrArm, data=ERSPC, ratio = 100)
eModel <- casebase::fitSmoothHazard(DeadOfPrCa ~ ScrArm, data=ERSPC, ratio = 100)
wModel <- casebase::fitSmoothHazard(DeadOfPrCa ~ log(Follow.Up.Time)
+ ScrArm, data=ERSPC, ratio = 100)
sModel <- casebase::fitSmoothHazard(DeadOfPrCa ~ splines::bs(Follow.Up.Time) + ScrArm, data=ERSPC, ratio = 100)
coxModel<-survival::coxph(Surv(Follow.Up.Time,DeadOfPrCa ) ~ ScrArm, data=ERSPC)
compareHazard1=as.data.frame(cbind(coxModel$coefficients, gModel$coefficients[3] ,eModel$coefficients[2],wModel$coefficients[3]))

compareHazard1=as.data.frame(rbind(coxModel$coefficients, gModel$coefficients[3] ,eModel$coefficients[2], wModel$coefficients[3]))
compareHazard2=as.data.frame(rbind(summary(coxModel)$coefficients[3], summary(gModel)$coefficients[3,2],summary(eModel)$coefficients[2,2], summary(wModel)$coefficients[3,2] ))
compareHazard1=round_df(exp(compareHazard1),3)
compareHazard2=round_df(exp(compareHazard2),3)
names=c("Cox","Gompertz","Exponential","Weibull")
compareHazard=cbind(names,compareHazard1,compareHazard2)
colnames(compareHazard)=c('Model',"Hazard Ratio","Std.Error")


knitr::kable(compareHazard,row.names = FALSE)
```


## Absolute Risk
* We have a bunch of different parametric hazard models now.
* To get the absolute risk, we need to evaluate the following equation in relation to the hazard:
$$CI(x,t)=1-e^{-\int^{t}_{0}h(x,u)du}$$
* *CI(x,t)*= Cumulative Incidence (Absolute Risk)
* *h(x,u)*= Hazard function

* Lets use the weibull hazard


## Casebase: Absolute Risk comparison
```{r eval=FALSE,echo=TRUE}
casebase::absoluteRisk(fit, time=5, covariate_profile)
```


```{r echo=FALSE,warning=FALSE,message=FALSE}
time_points <- seq(0,15, 1)
newData=as.data.frame(cbind("Control group",2,0))
colnames(newData)=c("ScrArm","Follow.Up.Time","DeadOfPrCa")
#newData$ScrArm=factor(newData$ScrArm)
#levels(newData$ScrArm)=c(0,1)
# calculate cumulative incidence using casebase model
wRisk <- absoluteRisk(object = wModel,time = time_points,newdata = newData)
#plot(wRisk)
coxRisk=survfit(coxModel, newdata = newData)
#plot(coxRisk$time,coxRisk$cumhaz)


# cumulative incidence function for the Cox model
plot(coxRisk$time,coxRisk$cumhaz, type="l",lwd=2,
xlab = "Years", ylab = "Cumulative Incidence (%)", fun = "event",
xlim = c(0,15), conf.int = F, col = "red", 
main = sprintf("Estimated Cumulative Incidence (risk) With No Screening"),cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)

# add casebase curve with legend
lines(wRisk[,1], wRisk[,2], type = "l", col = "blue",lwd=2)
legend("bottomright", 
legend = c("semi-parametric (Cox)", "parametric (casebase)"), 
col = c("red","blue"),
lty = c(1, 1), 
bg = "gray90",cex=1.5)

```

## Summary

* Casebase sampling implicitly incorporates censoring and permits the use of GLMs and the tools associated with them
* The casebase package contains tools to generate:
    * Population-Time plots
    * Hazard functions
    * Absolute Risk
    * Casebase can deal with competing risks.


## References 1

1.Hanley, James A, and Olli S Miettinen. 2009. "Fitting Smooth-in-Time Prognostic Risk Functions via Logistic Regression." *The International Journal of Biostatistics 5 (1)*.

2.Saarela, Olli, and Elja Arjas. 2015. "Non-Parametric Bayesian Hazard Regression for Chronic Disease Risk Assessment." Scandinavian Journal of Statistics 42 (2). Wiley Online Library: 609–26. 

3.Saarela, Olli. 2015. "A Case-Base Sampling Method for Estimating Recurrent Event Intensities." *Lifetime Data Analysis*. Springer, 1–17

## References 2

4.Schroder FH, et al., for the ERSPC Investigators.Screening and Prostate-Cancer Mortality in a Randomized European Study. *N Engl J Med* 2009;360:1320-8.

5.Scrucca L, Santucci A, Aversa F. Competing risk analysis using R: an easy guide for clinicians. *Bone Marrow Transplant*. 2007 Aug;40(4):381-7. doi: 10.1038/sj.bmt.1705727.

6.Turgeon, M. (2017, June 10). Retrieved May 05, 2019, from https://www.maxturgeon.ca/slides/MTurgeon-2017-Student-Conference.pdf

## Tutorial and Slides
\begin{center}
Tutorial:

http://sahirbhatnagar.com/casebase/

Slides:

https://github.com/Jesse-Islam/UseR--CaseBase-Presentation


Questions?
\end{center}

